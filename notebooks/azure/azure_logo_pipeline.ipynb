{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "sys.path += [str(Path(os.getcwd()).parent.parent / \"model_training\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "# how long dose it take to build a 'image_index' \n",
    "# -> lode images -> make vectors    --> SSIndex \n",
    "#                   make meta table _/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw train, val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logo_datasetpath = \"/home/schulz-kees/data/datasets/logo_dataset/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw meta data\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from siuba import _, group_by, summarize, mutate\n",
    "\n",
    "\n",
    "def load_dataset(root_path):\n",
    "    df = pd.concat([pd.read_parquet(x) for x in glob(f\"{root_path}/*/*.parquet\")]).reset_index(drop=True)\n",
    "    return (df >> mutate(path= _.class_id + \"/\" + _.image_id))\n",
    "#logo_image_df = load_dataset(root_logo_datasetpath)\n",
    "\n",
    "logo_class_df = pd.read_parquet(glob(f\"{root_logo_datasetpath}/*.parquet\")[0]).reset_index(drop=True)[[\"class_id\", \"logonummer\",\"variant\", \"name\", \"length\", \"width\",\"height\",\"volume_bottles\", \"num_bottles\",\"description\"]]\n",
    "\n",
    "\n",
    "#logo_image_df.to_parquet(\"./data/logo_image_df.parquet\")\n",
    "#logo_class_df.to_parquet(\"./data/logo_class_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test val\n",
    "from model_training.lib.keesds import get_all_values_geq_n, split, split_by_col_values\n",
    "from siuba import filter as _filter, summarize, _\n",
    "\n",
    "# i don't know a good name.. \n",
    "def gen_train_val_test(df):\n",
    "    values = get_all_values_geq_n(df, \"class_id\", 15)\n",
    "    df_more_then_15, rest1 = split_by_col_values(df, \"class_id\", values)\n",
    "\n",
    "    train_df, rest2 = split(df_more_then_15, \"class_id\", 10)\n",
    "    validate_df, rest3 = split(rest2, \"class_id\", 2)\n",
    "    test_df, _  = split(rest3, \"class_id\", 3)\n",
    "\n",
    "    values = get_all_values_geq_n(rest1, \"class_id\", 3)\n",
    "    df_more_then_3, _ = split_by_col_values(rest1, \"class_id\", values)\n",
    "\n",
    "    test_df_unknown, _  = split(df_more_then_3, \"class_id\", 3)\n",
    "\n",
    "    return train_df, validate_df, pd.concat([test_df, test_df_unknown]).reset_index(drop=True)\n",
    "\n",
    "\n",
    " \n",
    "train_df, validate_df, test_df = gen_train_val_test(logo_image_df)\n",
    "#train_df.to_parquet(\"./data/logo_train.parquet\")\n",
    "#validate_df.to_parquet(\"./data/logo_validate.parquet\")\n",
    "#test_df.to_parquet(\"./data/logo_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "logo_image_df = pd.read_parquet(\"./data/logo_image_df.parquet\")\n",
    "logo_class_df = pd.read_parquet(\"./data/logo_class_df.parquet\")\n",
    "\n",
    "train_df = pd.read_parquet(\"./data/logo_train.parquet\")\n",
    "validate_df = pd.read_parquet(\"./data/logo_validate.parquet\")\n",
    "test_df = pd.read_parquet(\"./data/logo_test.parquet\")\n",
    "\n",
    "\n",
    "if any(train_df[\"image_id\"].isin(validate_df[\"image_id\"])):\n",
    "    print(\"1 not allwed\")\n",
    "\n",
    "if any(train_df[\"image_id\"].isin(test_df[\"image_id\"])):\n",
    "    print(\"2 not allwed\")\n",
    "\n",
    "if any(test_df[\"image_id\"].isin(validate_df[\"image_id\"])):\n",
    "    print(\"3 not allwed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training.lib.keesds import fillna_by_col, combin_cols\n",
    "\n",
    "from siuba import _, mutate, filter as _filter\n",
    "from siuba.siu import symbolic_dispatch\n",
    "from typing import List\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "@symbolic_dispatch\n",
    "def get_geometry(a, b):\n",
    "    def inner(a,b):\n",
    "        if math.isnan(a):\n",
    "            return b\n",
    "        else:\n",
    "            return a\n",
    "    return [inner(x,y) for x,y in zip(a, b)]\n",
    "\n",
    "\n",
    "def fill_missing_values(df, cols, cols_alternative, label = \"class_id\"):\n",
    "    for col, a_col in zip(cols, cols_alternative):\n",
    "        fillna_by_col(df, \"class_id\", col)\n",
    "        df[col] = get_geometry(df[col], df[a_col])\n",
    "    \n",
    "    return df >> mutate(geometry=combin_cols(*[df[x] for x in cols]))\n",
    "\n",
    "\n",
    "# clean train df...\n",
    "def clean_df(df) -> pd.DataFrame:\n",
    "    df = pd.merge(df, logo_class_df, on=\"class_id\", how=\"left\")\n",
    "    df_clean = fill_missing_values(df, ['kastenbreite','kastenhoehe', 'kastenlaenge'],\n",
    "                            ['width', 'height', 'length'])\n",
    "    return (df_clean >> _filter(~_.kastenlaenge.isnull())).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All-NaN slice encountered\n"
     ]
    }
   ],
   "source": [
    "extra_classes_path = \"/home/schulz-kees/data/github/model_training/notebooks/models/data/logo_add_extra_classes.parquet\"\n",
    "\n",
    "\n",
    "clean_df(pd.read_parquet(extra_classes_path)).to_parquet(\"/home/schulz-kees/data/github/model_training/notebooks/models/data/logo_add_extra_classes_clean.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schulz-kees/.cache/pypoetry/virtualenvs/model-training-L-UrGKZC-py3.8/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1119: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    }
   ],
   "source": [
    "#print(len(validate_df))\n",
    "#df_train_clean = clean_df(train_df)\n",
    "#df_train_clean.to_parquet(\"./data/train_test_df.parquet\")\n",
    "clean_df(validate_df)[:50].to_parquet(\"./data/val_test_df_klein.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All-NaN slice encountered\n"
     ]
    }
   ],
   "source": [
    "#clean_df(validate_df).to_parquet(\"./data/val_test_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "EXPERIMENT_NAME = \"test_\"\n",
    "COMPUTE_TARGET = \"rohan\"#\"rohan\" #glamdring #mordor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, ScriptRunConfig, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# get workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# get data from workspace (\"folder\")\n",
    "ds = ws.get_default_datastore()\n",
    "root_data_path = ds.path('logo_dataset_v1-beta/data/').as_mount()\n",
    "train_dataset_path = ds.path(\"logo_tmp/train_test_df.parquet\").as_mount()\n",
    "validate_dataset_path = ds.path(\"logo_tmp/val_test_df.parquet\").as_mount()\n",
    "\n",
    "\n",
    "\n",
    "# config environment (https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments)\n",
    "env = Environment.get(ws, name=\"gpu_test_env\") \n",
    "# https://github.com/Azure/MachineLearningNotebooks/issues/1483\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "dc = DockerConfiguration(use_docker=True, shm_size='30g')\n",
    "\n",
    "# config script run\n",
    "s1 = ScriptRunConfig(\n",
    "    source_directory='../../src/',\n",
    "    script='./scripts/train_script.py',\n",
    "    compute_target=COMPUTE_TARGET,\n",
    "    environment=env,\n",
    "    docker_runtime_config=dc,\n",
    "    arguments = [\n",
    "        '--root_data_path', str(root_data_path),\n",
    "        '--train_dataset_path', str(train_dataset_path),\n",
    "        '--validation_dataset_path', str(validate_dataset_path),\n",
    "        '--max_epochs', 600,   # number of max epochs...\n",
    "        '--num_samples_epoch', 250, # number of triplets in 1 epoch \n",
    "        '--interval_update_index', 16, # every x epochs the index gets updated\n",
    "        '--batch_size', 15, # test to set it to 15?\n",
    "        '--num_workers', 4,\n",
    "        '--gamma', 0.05,\n",
    "        '--ss_index_name', \"HD_fdcc2c00-9114-4106-ae69-2f38620f3f5c_4\",\n",
    "        '--ss_index_version', 37,\n",
    "        '--model_name', \"HD_fdcc2c00-9114-4106-ae69-2f38620f3f5c_4.ptm\",\n",
    "        '--model_version', 37,\n",
    "        '--max_loss', 100,\n",
    "        '--freeze', 17\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/data/\n",
    "s1.run_config.data_references[root_data_path.data_reference_name] = root_data_path.to_config()\n",
    "s1.run_config.data_references[train_dataset_path.data_reference_name] = train_dataset_path.to_config()\n",
    "s1.run_config.data_references[validate_dataset_path.data_reference_name] = validate_dataset_path.to_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypterdive\n",
    "from azureml.train.hyperdrive import normal, uniform, RandomParameterSampling, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "\n",
    "param_space = {\n",
    "                 '--lr': uniform(0.0005, 0.0025),\n",
    "                 '--margin': uniform(5, 15),\n",
    "                 '--loss_weight': uniform(0.30, 0.80)\n",
    "              }\n",
    "\n",
    "param_sampling = RandomParameterSampling(param_space)\n",
    "\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=5)\n",
    "\n",
    "hd_config = HyperDriveConfig(run_config=s1,\n",
    "                             hyperparameter_sampling=param_sampling,\n",
    "                             policy=None,\n",
    "                             primary_metric_name=\"class_acc\",\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                             max_total_runs=8,\n",
    "                             max_concurrent_runs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiment\n",
    "\n",
    "from azureml.core.experiment import Experiment\n",
    "#from azureml.pipeline.core.pipeline import Pipeline\n",
    "# Copy src to a tmp src for Azure\n",
    "#\n",
    "\n",
    "experiment = Experiment(ws, EXPERIMENT_NAME)\n",
    "# Build the Pipeline from the Steps \n",
    "#steps = [s1_hypesr_train]\n",
    "\n",
    "#pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "# Run the Pipeline\n",
    "#pipeline_run = experiment.submit(s1)\n",
    "pipeline_run = experiment.submit(hd_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuda memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "import gc\n",
    "#del variables\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# https://deeptechtalker.medium.com/pytorch-cant-allocate-more-memory-1c36d7c9df4\n",
    "# https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36103/1525434380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_resnet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogoModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_resnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/github/model_training/model_training/_script_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_resnet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_ss_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_ss_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_ss_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSSIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_model'"
     ]
    }
   ],
   "source": [
    "from model_training._script_util import get_children\n",
    "from model_training._model import LogoModel, get_resnet18\n",
    "\n",
    "get_children(LogoModel(get_resnet18(cut=1)))[-17:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add private python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-private-python-packages\n",
    "\n",
    "#whl_url = Environment.add_private_pip_wheel(workspace=ws,file_path = \"../../libs/indexmodel-0.3.1-py3-none-any.whl\")\n",
    "myenv = Environment(name=\"gpu_test_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imageExistsInRegistry': False, 'intellectualPropertyPublisher': None, 'imageCapabilities': {'canAccessData': True}, 'ingredients': {'dockerfile': 'FROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20211111.v1\\n\\nENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/pytorch-1.10\\n\\n# Create conda environment\\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\\\\n    python=3.8 \\\\\\n    pip=20.2.4 \\\\\\n    pytorch=1.10.0 \\\\\\n    torchvision=0.11.1 \\\\\\n    cudatoolkit=11.1.1 \\\\\\n    nvidia-apex=0.1.0 \\\\\\n    gxx_linux-64 \\\\\\n    -c anaconda -c pytorch -c conda-forge\\n\\n# Prepend path to AzureML conda environment\\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\\n\\n# Install pip dependencies\\nRUN pip install \\'matplotlib>=3.3,<3.4\\' \\\\\\n                \\'psutil>=5.8,<5.9\\' \\\\\\n                \\'tqdm>=4.59,<4.63\\' \\\\\\n                \\'pandas>=1.3,<1.4\\' \\\\\\n                \\'scipy>=1.5,<1.8\\' \\\\\\n                \\'numpy>=1.10,<1.22\\' \\\\\\n                \\'azureml-core==1.36.0.post2\\' \\\\\\n                \\'azureml-defaults==1.36.0\\' \\\\\\n                \\'azureml-mlflow==1.36.0\\' \\\\\\n                \\'azureml-telemetry==1.36.0\\' \\\\\\n                \\'onnxruntime-gpu>=1.7,<1.10\\' \\\\\\n                \\'horovod==0.23\\' \\\\\\n                \\'future==0.18.2\\' \\\\\\n                \\'torch-tb-profiler==0.3.1\\'\\\\\\n                \\'faiss-cpu==1.7.1\\'\\\\\\n                \\'click==8.0.3\\'\\\\\\n                \\'pyarrow>=6.0.0\\'\\\\\\n                \\'fastparquet>=0.7.1\\'\\\\\\n                \\'pytorch-ignite>=0.4.7\\'\\\\\\n                \\'pydantic>=1.8.2\\'\\\\\\n                \\'siuba>=0.0.25\\'\\\\\\n                \\'ml2rt>=0.2.0\\'\\\\\\n                \\'scikit-image>=0.18.2\\'\\\\\\n                \\'azureml-train-core>=1.36.0\\'\\\\\\n                \\'GPUtil>=1.4.0\\'\\\\\\n                \\'https://shire5329111891.blob.core.windows.net/azureml/Environment/azureml-private-packages/indexmodel-0.3.1-py3-none-any.whl\\'\\n\\n\\n# This is needed for mpi to locate libpython\\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\\nUSER root\\nRUN mkdir -p $HOME/.cache\\nWORKDIR /\\nCOPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\\nCOPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\\nRUN if [ $SPARK_HOME ]; then /bin/bash -c \\'$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py\\'; fi\\nRUN rm -rf azureml-environment-setup\\nENV AZUREML_ENVIRONMENT_IMAGE True\\nCMD [\"bash\"]\\n', 'condaSpecification': None}, 'pythonEnvironment': {'interpreterPath': 'python', 'condaEnvironmentName': None, 'condaEnvironmentPath': None}, 'dockerImage': {'name': 'azureml/azureml_c1f026830f15e91adcdab3f83a4d877b', 'registry': {'address': 'shire626d1996.azurecr.io', 'username': 'shi***', 'password': '***'}}, 'warnings': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myenv.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://shire5329111891.blob.core.windows.net/azureml/Environment/azureml-private-packages/indexmodel-0.3.1-py3-none-any.whl'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whl_url"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39e8011ad54174455f0b6de30e8a10c44d094cf0b69ba87903b88334cd7fa3ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('model-training-L-UrGKZC-py3.8': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
